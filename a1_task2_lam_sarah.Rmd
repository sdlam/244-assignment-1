---
title: "Assignment 1 Task 2"
author: "Sarah Lam"
output: html_document
---

```{r setup, include=FALSE, waring = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(here)
library(janitor)
library(kableExtra)
library(GGally)
library(AICcmodavg)
library(equatiomatic)
```

#### Overview 

This code explores the relationship between O2 saturation of seawater off Californiaâ€™s coast and several physical and chemical variables using data from CalCOFI to compare competing linear regression models using AIC and cross validation. 

Data citation: CalCOFI data are available for use without restriction. Data downloaded from https://calcofi.org/ccdata.html.  Accessed 1/10/2022.

```{r}
cal_seawater <- read_csv(here("data", "calcofi_seawater_samples.csv"))
```

```{r, include = FALSE}
cal_seawater %>% # explore variables and linearity 
  select(o2sat:no2u_m) %>% 
  ggpairs()
```

#### Linear Regression Models 

```{r}
o2_f1 <- o2sat ~ t_deg_c + salinity + po4u_m

o2_f2 <- o2sat ~ t_deg_c + salinity + po4u_m + depth_m

o2_lm1 <- lm(data = cal_seawater, o2_f1)

o2_lm2 <- lm(data = cal_seawater, o2_f2)
```

##### Akaike Information Criterion 

```{r}
#AIC 
AICc(o2_lm1) #corrected AIC, in AICcmodavg pkg
AICc(o2_lm2)
 
aictab <- aictab(list(o2_lm1, o2_lm2))
```

Based on the AICc values, the second linear regression model (`r round(aictab$AICc[1], 2)`) is a better model than the first linear regression model (`r round(aictab$AICc[2], 2)`). The difference in AICcs is `r round(aictab$AICc[2] - aictab$AICc[1], 2)`. This reveals that the added parameter of depth significantly imporved the model's fit compared to a model omitting depth as a parameter.

##### K-fold Cross Validation

```{r}
#k-fold cross validation
folds <- 10
fold_vec <- rep(1:folds, length.out = nrow(cal_seawater))
 
set.seed(42) 
 
o2_seawater_fold <- cal_seawater %>%
  mutate(group = sample(fold_vec, size = n(), replace = FALSE))
table(o2_seawater_fold$group)
 
# code the first fold
test_df <- o2_seawater_fold %>%
  filter(group == 1)
 
train_df <- o2_seawater_fold %>%
  filter(group != 1)

#store root mean square error for later predicting which model predicts better
calc_rmse <- function(x, y) {
  rmse <- (x - y)^2 %>% mean() %>% sqrt()
  return(rmse)
}

#create linear models using training dataset 
training_mdl1 <- lm(o2_f1, data = train_df)
training_mdl2 <- lm(o2_f2, data = train_df)

#use models to predict o2 concentration in test dataset
predict_test <- test_df %>%
  mutate(model1 = predict(training_mdl1, test_df),
         model2 = predict(training_mdl2, test_df))

#Use root means to determine better model
rmse_predict_test <- predict_test %>%
  summarize(rmse_mdl1 = calc_rmse(model1, o2sat),
            rmse_mdl2 = calc_rmse(model2, o2sat))
 
rmse_predict_test
```

```{r}
# train model on whole dataset rather than just a single fold
rmse_df <- data.frame()
 
for(i in 1:folds) {
  # i <- 1
  kfold_test_df <- o2_seawater_fold %>%
    filter(group == i)
  kfold_train_df <- o2_seawater_fold %>%
    filter(group != i)
  
  kfold_mdl1 <- lm(o2_f1, data = kfold_train_df)
  kfold_mdl2 <- lm(o2_f2, data = kfold_train_df)
 

  kfold_pred_df <- kfold_test_df %>%
    mutate(mdl1 = predict(kfold_mdl1, kfold_test_df),
           mdl2 = predict(kfold_mdl2, .)) 
  kfold_rmse <- kfold_pred_df %>%
    summarize(rmse_mdl1 = calc_rmse(mdl1, o2sat),
              rmse_mdl2 = calc_rmse(mdl2, o2sat),
              test_gp = i)
  
  rmse_df <- bind_rows(rmse_df, kfold_rmse)
}
 
rmse_df

#root means for each model to assess superiority 
rmse_df %>% 
  summarize(mean_rmse_mdl1 = mean(rmse_mdl1),
            mean_rmse_mdl2 = mean(rmse_mdl2))
```

Based on the results of the 10-fold cross validation, we see that model 2 does a slightly better job of predicting oxygen saturation (lower error) than model 1, this aligns with out AIC results as well.

Here the various models are very close in performance.  Which to use?  AIC and cross-validation both indicate model 2, though this isn't always the case.  If you're using your model to predict on new data, CV is probably the better way to go, though if your data set is small, AIC is probably better.

So we will use the entire dataset, rather than testing/training sets, to identify the coefficients for the final predictive model, based on model 2.  We already did this earlier, but let's do it again just to make the point.

```{r}
final_mdl <- lm(o2_f2, data = cal_seawater)
summary(final_mdl)
```

Our final model:
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE)`

and with coefficients in place:
`r equatiomatic::extract_eq(final_mdl, wrap = TRUE, use_coefs = TRUE)`

